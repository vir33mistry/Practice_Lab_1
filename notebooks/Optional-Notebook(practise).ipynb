{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance with Linear Regression-Based Alerts\n",
    "\n",
    "**Author:** Param Avinashkumar Rasaniya\n",
    "**Student ID:** 9086095\n",
    "**Course:** CSCN8010-26W-Sec1-Foundations of Machine Learning Frameworks \n",
    "**Practical Lab 1:**  Streaming Data for Predictive Maintenance with Linear Regression-Based Alerts\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. Connect to Neon.tech PostgreSQL database\n",
    "2. Load and explore training data\n",
    "3. Build linear regression models for each axis\n",
    "4. Analyze residuals to discover thresholds\n",
    "5. Generate synthetic test data\n",
    "6. Implement alert/error detection\n",
    "7. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection Setup\n",
    "\n",
    "### Steps to set up Neon.tech database:\n",
    "\n",
    "1. **Create Neon Account**: Go to https://neon.tech and sign up\n",
    "2. **Create New Project**: Click \"New Project\" in the dashboard\n",
    "3. **Get Connection String**: Copy your connection details\n",
    "4. **Create .env file**: Add credentials (see README.md)\n",
    "\n",
    "Your connection string will look like:\n",
    "```\n",
    "postgresql://user:password@ep-xxx.region.aws.neon.tech/dbname?sslmode=require\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Database configuration\n",
    "# Database configuration - Direct credentials\n",
    "DB_CONFIG = {\n",
    "    'host': 'ep-dark-base-aiac0jg1-pooler.c-4.us-east-1.aws.neon.tech',\n",
    "    'database': 'neondb',\n",
    "    'user': 'neondb_owner',\n",
    "    'password': 'npg_oVLxIZk49eiX',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "print(\"✓ Database configuration loaded\")\n",
    "print(f\"Connecting to: {DB_CONFIG['host']}\")\n",
    "\n",
    "def connect_to_database():\n",
    "    \"\"\"Establish connection to Neon PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        print(\"✓ Successfully connected to Neon.tech database\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Database connection failed: {e}\")\n",
    "        print(\"\\nPlease check:\")\n",
    "        print(\"1. .env file exists with correct credentials\")\n",
    "        print(\"2. Neon.tech database is active\")\n",
    "        print(\"3. Network connection is stable\")\n",
    "        return None\n",
    "\n",
    "# Test connection\n",
    "connection = connect_to_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Database Table and Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn):\n",
    "    \"\"\"Create table for current readings if it doesn't exist\"\"\"\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS current_readings (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        timestamp TIMESTAMP,\n",
    "        axis_1 FLOAT,\n",
    "        axis_2 FLOAT,\n",
    "        axis_3 FLOAT,\n",
    "        axis_4 FLOAT,\n",
    "        axis_5 FLOAT,\n",
    "        axis_6 FLOAT,\n",
    "        axis_7 FLOAT,\n",
    "        axis_8 FLOAT\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        print(\"✓ Table 'current_readings' created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating table: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def load_csv_to_database(conn, csv_path):\n",
    "    \"\"\"Load training data from CSV into database\"\"\"\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\College\\CSCN8010\\Practical_Lab_1\\data\\RMBR4-2_export_test.csv\")\n",
    "        \n",
    "        # Clean and prepare data\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "        \n",
    "        # Select only the first 8 axes\n",
    "        axes_cols = [f'Axis #{i}' for i in range(1, 9)]\n",
    "        df_clean = df[['Time'] + axes_cols].copy()\n",
    "        df_clean.columns = ['timestamp'] + [f'axis_{i}' for i in range(1, 9)]\n",
    "        \n",
    "        # Convert to numeric, handling any errors\n",
    "        for col in df_clean.columns[1:]:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        \n",
    "        # Remove rows with all zero values or NaN\n",
    "        df_clean = df_clean[(df_clean.iloc[:, 1:] != 0).any(axis=1)]\n",
    "        df_clean = df_clean.dropna()\n",
    "        \n",
    "        # Insert into database\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Clear existing data\n",
    "        cursor.execute(\"DELETE FROM current_readings\")\n",
    "        \n",
    "        # Insert new data\n",
    "        for _, row in df_clean.iterrows():\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO current_readings (timestamp, axis_1, axis_2, axis_3, axis_4, \n",
    "                                         axis_5, axis_6, axis_7, axis_8)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, tuple(row))\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "        print(f\"✓ Loaded {len(df_clean)} records into database\")\n",
    "        return df_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "\n",
    "# Create table and load data\n",
    "if connection:\n",
    "    create_table(connection)\n",
    "    training_data = load_csv_to_database(connection, r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\College\\CSCN8010\\Practical_Lab_1\\data\\RMBR4-2_export_test.csv\")\n",
    "    \n",
    "    if training_data is not None:\n",
    "        print(f\"\\nData shape: {training_data.shape}\")\n",
    "        print(f\"Date range: {training_data['timestamp'].min()} to {training_data['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_training_data(conn):\n",
    "    \"\"\"Query all training data from database\"\"\"\n",
    "    query = \"SELECT * FROM current_readings ORDER BY timestamp\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        print(f\"✓ Retrieved {len(df)} records from database\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error querying data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Query training data\n",
    "if connection:\n",
    "    df_train = query_training_data(connection)\n",
    "    \n",
    "    if df_train is not None:\n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst 5 rows of training data:\")\n",
    "        display(df_train.head())\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        display(df_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based features\n",
    "df_train['time_seconds'] = (df_train['timestamp'] - df_train['timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "# Visualize raw data for all axes\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(1, 9):\n",
    "    ax = axes[i-1]\n",
    "    ax.plot(df_train['time_seconds'], df_train[f'axis_{i}'], alpha=0.6, linewidth=0.8)\n",
    "    ax.set_title(f'Axis #{i} - Raw Current Data')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Current (kWh)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/raw_data_all_axes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Raw data visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Linear Regression Models\n",
    "\n",
    "We fit a univariate linear regression model for each axis:\n",
    "$$\\text{Current} = \\beta_0 + \\beta_1 \\times \\text{Time} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and statistics\n",
    "models = {}\n",
    "model_stats = []\n",
    "\n",
    "# Prepare time feature\n",
    "X = df_train[['time_seconds']].values\n",
    "\n",
    "# Fit regression model for each axis\n",
    "for i in range(1, 9):\n",
    "    axis_col = f'axis_{i}'\n",
    "    y = df_train[axis_col].values\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Calculate R² score\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    # Store model and statistics\n",
    "    models[f'axis_{i}'] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'residuals': residuals,\n",
    "        'slope': model.coef_[0],\n",
    "        'intercept': model.intercept_,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'residual_mean': np.mean(residuals),\n",
    "        'residual_std': np.std(residuals)\n",
    "    }\n",
    "    \n",
    "    model_stats.append({\n",
    "        'Axis': f'#{i}',\n",
    "        'Slope': f\"{model.coef_[0]:.6f}\",\n",
    "        'Intercept': f\"{model.intercept_:.4f}\",\n",
    "        'R²': f\"{r2:.4f}\",\n",
    "        'RMSE': f\"{rmse:.4f}\",\n",
    "        'Residual Mean': f\"{np.mean(residuals):.6f}\",\n",
    "        'Residual Std': f\"{np.std(residuals):.4f}\"\n",
    "    })\n",
    "\n",
    "# Display model statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINEAR REGRESSION MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "stats_df = pd.DataFrame(model_stats)\n",
    "display(stats_df)\n",
    "\n",
    "print(\"\\n✓ Successfully trained 8 linear regression models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Regression Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regression lines for all axes\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "X_plot = df_train['time_seconds'].values\n",
    "\n",
    "for i in range(1, 9):\n",
    "    ax = axes[i-1]\n",
    "    axis_col = f'axis_{i}'\n",
    "    \n",
    "    # Plot actual data\n",
    "    ax.scatter(X_plot, df_train[axis_col], alpha=0.3, s=10, label='Actual', color='blue')\n",
    "    \n",
    "    # Plot regression line\n",
    "    y_pred = models[axis_col]['predictions']\n",
    "    ax.plot(X_plot, y_pred, 'r-', linewidth=2, label='Regression Line')\n",
    "    \n",
    "    # Add equation to plot\n",
    "    slope = models[axis_col]['slope']\n",
    "    intercept = models[axis_col]['intercept']\n",
    "    r2 = models[axis_col]['r2']\n",
    "    \n",
    "    equation = f'y = {slope:.6f}x + {intercept:.4f}\\nR² = {r2:.4f}'\n",
    "    ax.text(0.05, 0.95, equation, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "            fontsize=8)\n",
    "    \n",
    "    ax.set_title(f'Axis #{i} - Regression Model')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Current (kWh)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/regression_lines_all_axes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Regression visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis\n",
    "\n",
    "Analyze residuals to discover optimal thresholds for alerts and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual distributions\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(1, 9):\n",
    "    ax = axes[i-1]\n",
    "    axis_col = f'axis_{i}'\n",
    "    residuals = models[axis_col]['residuals']\n",
    "    \n",
    "    # Create histogram with KDE\n",
    "    ax.hist(residuals, bins=50, alpha=0.6, color='steelblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Overlay KDE\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(residuals)\n",
    "    x_range = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Add mean and std lines\n",
    "    mean = np.mean(residuals)\n",
    "    std = np.std(residuals)\n",
    "    \n",
    "    ax.axvline(mean, color='green', linestyle='--', linewidth=2, label=f'Mean = {mean:.4f}')\n",
    "    ax.axvline(mean + 2*std, color='orange', linestyle='--', linewidth=2, label=f'Mean + 2σ')\n",
    "    ax.axvline(mean + 3*std, color='red', linestyle='--', linewidth=2, label=f'Mean + 3σ')\n",
    "    \n",
    "    ax.set_title(f'Axis #{i} - Residual Distribution')\n",
    "    ax.set_xlabel('Residual (kWh)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/residual_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Residual analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold Discovery\n",
    "\n",
    "Based on residual analysis, we determine thresholds using statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thresholds for each axis\n",
    "thresholds = {}\n",
    "threshold_stats = []\n",
    "\n",
    "for i in range(1, 9):\n",
    "    axis_col = f'axis_{i}'\n",
    "    residuals = models[axis_col]['residuals']\n",
    "    \n",
    "    mean_res = np.mean(residuals)\n",
    "    std_res = np.std(residuals)\n",
    "    \n",
    "    # Define thresholds\n",
    "    # Alert: Mean + 2 standard deviations (95% confidence)\n",
    "    # Error: Mean + 3 standard deviations (99.7% confidence)\n",
    "    min_c = mean_res + 2 * std_res  # Alert threshold\n",
    "    max_c = mean_res + 3 * std_res  # Error threshold\n",
    "    \n",
    "    thresholds[axis_col] = {\n",
    "        'MinC': min_c,\n",
    "        'MaxC': max_c,\n",
    "        'T': 10  # 10 seconds continuous deviation\n",
    "    }\n",
    "    \n",
    "    # Count outliers\n",
    "    alert_count = np.sum(residuals > min_c)\n",
    "    error_count = np.sum(residuals > max_c)\n",
    "    \n",
    "    threshold_stats.append({\n",
    "        'Axis': f'#{i}',\n",
    "        'MinC (Alert)': f\"{min_c:.4f}\",\n",
    "        'MaxC (Error)': f\"{max_c:.4f}\",\n",
    "        'T (seconds)': 10,\n",
    "        'Potential Alerts': alert_count,\n",
    "        'Potential Errors': error_count,\n",
    "        'Alert %': f\"{(alert_count/len(residuals)*100):.2f}%\",\n",
    "        'Error %': f\"{(error_count/len(residuals)*100):.2f}%\"\n",
    "    })\n",
    "\n",
    "# Display threshold statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DISCOVERED THRESHOLDS (Based on Statistical Analysis)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nThreshold Definition:\")\n",
    "print(\"  MinC (Alert): Mean + 2σ - Captures 95% of normal variation\")\n",
    "print(\"  MaxC (Error): Mean + 3σ - Captures 99.7% of normal variation\")\n",
    "print(\"  T (Time Window): 10 seconds of continuous deviation\")\n",
    "print(\"\\n\")\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_stats)\n",
    "display(threshold_df)\n",
    "\n",
    "print(\"\\n✓ Thresholds discovered and validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Synthetic Test Data\n",
    "\n",
    "Create test data with similar statistical properties to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_test_data(train_df, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic test data based on training data statistics.\n",
    "    Adds some anomalies to test alert/error detection.\n",
    "    \"\"\"\n",
    "    # Start from last timestamp in training data\n",
    "    start_time = train_df['timestamp'].max() + pd.Timedelta(seconds=2)\n",
    "    \n",
    "    # Generate timestamps\n",
    "    timestamps = [start_time + pd.Timedelta(seconds=i*2) for i in range(num_samples)]\n",
    "    \n",
    "    # Initialize test data\n",
    "    test_data = {'timestamp': timestamps}\n",
    "    \n",
    "    # Generate data for each axis based on training statistics\n",
    "    for i in range(1, 9):\n",
    "        axis_col = f'axis_{i}'\n",
    "        \n",
    "        # Get training statistics\n",
    "        train_mean = train_df[axis_col].mean()\n",
    "        train_std = train_df[axis_col].std()\n",
    "        \n",
    "        # Generate base synthetic data (normal distribution)\n",
    "        synthetic_values = np.random.normal(train_mean, train_std, num_samples)\n",
    "        \n",
    "        # Add some anomalies (10% of data)\n",
    "        anomaly_indices = np.random.choice(num_samples, size=int(num_samples * 0.1), replace=False)\n",
    "        \n",
    "        for idx in anomaly_indices:\n",
    "            # Add significant positive deviation\n",
    "            synthetic_values[idx] += np.random.uniform(2 * train_std, 4 * train_std)\n",
    "        \n",
    "        # Ensure non-negative values\n",
    "        synthetic_values = np.maximum(synthetic_values, 0)\n",
    "        \n",
    "        test_data[axis_col] = synthetic_values\n",
    "    \n",
    "    # Create DataFrame\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Add time_seconds column\n",
    "    test_df['time_seconds'] = (test_df['timestamp'] - train_df['timestamp'].min()).dt.total_seconds()\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Generate test data\n",
    "df_test = generate_synthetic_test_data(df_train, num_samples=500)\n",
    "\n",
    "print(f\"✓ Generated {len(df_test)} test samples\")\n",
    "print(f\"\\nTest data date range: {df_test['timestamp'].min()} to {df_test['timestamp'].max()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "display(df_test.head())\n",
    "\n",
    "# Compare statistics\n",
    "print(\"\\nStatistical Comparison (Training vs Test):\")\n",
    "comparison_stats = []\n",
    "for i in range(1, 9):\n",
    "    axis_col = f'axis_{i}'\n",
    "    comparison_stats.append({\n",
    "        'Axis': f'#{i}',\n",
    "        'Train Mean': f\"{df_train[axis_col].mean():.4f}\",\n",
    "        'Test Mean': f\"{df_test[axis_col].mean():.4f}\",\n",
    "        'Train Std': f\"{df_train[axis_col].std():.4f}\",\n",
    "        'Test Std': f\"{df_test[axis_col].std():.4f}\"\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(comparison_stats))\n",
    "\n",
    "# Save to CSV\n",
    "df_test.to_csv('../data/synthetic_test_data.csv', index=False)\n",
    "print(\"\\n✓ Test data saved to '../data/synthetic_test_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Normalization and Standardization\n",
    "\n",
    "Apply Min-Max normalization and Z-score standardization to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization\n",
    "def apply_normalization(train_df, test_df, axis_columns):\n",
    "    \"\"\"\n",
    "    Apply Min-Max normalization using training data statistics.\n",
    "    Formula: X_norm = (X - X_min) / (X_max - X_min)\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit on training data\n",
    "    scaler.fit(train_df[axis_columns])\n",
    "    \n",
    "    # Transform test data\n",
    "    test_normalized = scaler.transform(test_df[axis_columns])\n",
    "    \n",
    "    # Create normalized DataFrame\n",
    "    df_normalized = test_df.copy()\n",
    "    df_normalized[axis_columns] = test_normalized\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "# Z-Score Standardization\n",
    "def apply_standardization(train_df, test_df, axis_columns):\n",
    "    \"\"\"\n",
    "    Apply Z-score standardization using training data statistics.\n",
    "    Formula: Z = (X - μ) / σ\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data\n",
    "    scaler.fit(train_df[axis_columns])\n",
    "    \n",
    "    # Transform test data\n",
    "    test_standardized = scaler.transform(test_df[axis_columns])\n",
    "    \n",
    "    # Create standardized DataFrame\n",
    "    df_standardized = test_df.copy()\n",
    "    df_standardized[axis_columns] = test_standardized\n",
    "    \n",
    "    return df_standardized, scaler\n",
    "\n",
    "# Apply transformations\n",
    "axis_columns = [f'axis_{i}' for i in range(1, 9)]\n",
    "\n",
    "df_test_normalized, norm_scaler = apply_normalization(df_train, df_test, axis_columns)\n",
    "df_test_standardized, std_scaler = apply_standardization(df_train, df_test, axis_columns)\n",
    "\n",
    "print(\"✓ Normalization and standardization complete\")\n",
    "print(\"\\nNormalized data sample:\")\n",
    "display(df_test_normalized[axis_columns].head())\n",
    "\n",
    "print(\"\\nStandardized data sample (Z-scores):\")\n",
    "display(df_test_standardized[axis_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Implement Alert and Error Detection\n",
    "\n",
    "Apply threshold-based anomaly detection on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(test_df, models, thresholds):\n",
    "    \"\"\"\n",
    "    Detect alerts and errors based on regression predictions and thresholds.\n",
    "    \n",
    "    Alert: Deviation ≥ MinC for ≥ T seconds\n",
    "    Error: Deviation ≥ MaxC for ≥ T seconds\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    errors = []\n",
    "    \n",
    "    X_test = test_df[['time_seconds']].values\n",
    "    \n",
    "    for i in range(1, 9):\n",
    "        axis_col = f'axis_{i}'\n",
    "        \n",
    "        # Get model and predict\n",
    "        model = models[axis_col]['model']\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_actual = test_df[axis_col].values\n",
    "        \n",
    "        # Calculate residuals (positive means above prediction)\n",
    "        residuals = y_actual - y_pred\n",
    "        \n",
    "        # Get thresholds\n",
    "        min_c = thresholds[axis_col]['MinC']\n",
    "        max_c = thresholds[axis_col]['MaxC']\n",
    "        T = thresholds[axis_col]['T']  # Time window in seconds\n",
    "        \n",
    "        # Detect continuous deviations\n",
    "        alert_flags = residuals > min_c\n",
    "        error_flags = residuals > max_c\n",
    "        \n",
    "        # Find continuous sequences\n",
    "        # For alerts\n",
    "        alert_start = None\n",
    "        for j in range(len(alert_flags)):\n",
    "            if alert_flags[j]:\n",
    "                if alert_start is None:\n",
    "                    alert_start = j\n",
    "            else:\n",
    "                if alert_start is not None:\n",
    "                    duration = (test_df.iloc[j-1]['timestamp'] - \n",
    "                               test_df.iloc[alert_start]['timestamp']).total_seconds()\n",
    "                    \n",
    "                    if duration >= T:\n",
    "                        alerts.append({\n",
    "                            'axis': f'#{i}',\n",
    "                            'start_time': test_df.iloc[alert_start]['timestamp'],\n",
    "                            'end_time': test_df.iloc[j-1]['timestamp'],\n",
    "                            'duration_seconds': duration,\n",
    "                            'max_deviation': residuals[alert_start:j].max(),\n",
    "                            'avg_deviation': residuals[alert_start:j].mean()\n",
    "                        })\n",
    "                    alert_start = None\n",
    "        \n",
    "        # Check last sequence for alerts\n",
    "        if alert_start is not None:\n",
    "            duration = (test_df.iloc[-1]['timestamp'] - \n",
    "                       test_df.iloc[alert_start]['timestamp']).total_seconds()\n",
    "            if duration >= T:\n",
    "                alerts.append({\n",
    "                    'axis': f'#{i}',\n",
    "                    'start_time': test_df.iloc[alert_start]['timestamp'],\n",
    "                    'end_time': test_df.iloc[-1]['timestamp'],\n",
    "                    'duration_seconds': duration,\n",
    "                    'max_deviation': residuals[alert_start:].max(),\n",
    "                    'avg_deviation': residuals[alert_start:].mean()\n",
    "                })\n",
    "        \n",
    "        # For errors (same logic)\n",
    "        error_start = None\n",
    "        for j in range(len(error_flags)):\n",
    "            if error_flags[j]:\n",
    "                if error_start is None:\n",
    "                    error_start = j\n",
    "            else:\n",
    "                if error_start is not None:\n",
    "                    duration = (test_df.iloc[j-1]['timestamp'] - \n",
    "                               test_df.iloc[error_start]['timestamp']).total_seconds()\n",
    "                    \n",
    "                    if duration >= T:\n",
    "                        errors.append({\n",
    "                            'axis': f'#{i}',\n",
    "                            'start_time': test_df.iloc[error_start]['timestamp'],\n",
    "                            'end_time': test_df.iloc[j-1]['timestamp'],\n",
    "                            'duration_seconds': duration,\n",
    "                            'max_deviation': residuals[error_start:j].max(),\n",
    "                            'avg_deviation': residuals[error_start:j].mean()\n",
    "                        })\n",
    "                    error_start = None\n",
    "        \n",
    "        # Check last sequence for errors\n",
    "        if error_start is not None:\n",
    "            duration = (test_df.iloc[-1]['timestamp'] - \n",
    "                       test_df.iloc[error_start]['timestamp']).total_seconds()\n",
    "            if duration >= T:\n",
    "                errors.append({\n",
    "                    'axis': f'#{i}',\n",
    "                    'start_time': test_df.iloc[error_start]['timestamp'],\n",
    "                    'end_time': test_df.iloc[-1]['timestamp'],\n",
    "                    'duration_seconds': duration,\n",
    "                    'max_deviation': residuals[error_start:].max(),\n",
    "                    'avg_deviation': residuals[error_start:].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(alerts), pd.DataFrame(errors)\n",
    "\n",
    "# Run anomaly detection\n",
    "df_alerts, df_errors = detect_anomalies(df_test, models, thresholds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Alerts Detected: {len(df_alerts)}\")\n",
    "print(f\"Total Errors Detected: {len(df_errors)}\")\n",
    "\n",
    "if len(df_alerts) > 0:\n",
    "    print(\"\\n--- ALERTS ---\")\n",
    "    display(df_alerts)\n",
    "    \n",
    "    # Save alerts\n",
    "    df_alerts.to_csv('../outputs/alerts_log.csv', index=False)\n",
    "    print(\"\\n✓ Alerts saved to '../outputs/alerts_log.csv'\")\n",
    "else:\n",
    "    print(\"\\nNo alerts detected in test data.\")\n",
    "\n",
    "if len(df_errors) > 0:\n",
    "    print(\"\\n--- ERRORS ---\")\n",
    "    display(df_errors)\n",
    "    \n",
    "    # Save errors\n",
    "    df_errors.to_csv('../outputs/errors_log.csv', index=False)\n",
    "    print(\"\\n✓ Errors saved to '../outputs/errors_log.csv'\")\n",
    "else:\n",
    "    print(\"\\nNo errors detected in test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Results with Alert/Error Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization with alerts and errors\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "X_test = df_test[['time_seconds']].values\n",
    "\n",
    "for i in range(1, 9):\n",
    "    ax = axes[i-1]\n",
    "    axis_col = f'axis_{i}'\n",
    "    \n",
    "    # Plot test data\n",
    "    ax.scatter(df_test['time_seconds'], df_test[axis_col], \n",
    "               alpha=0.4, s=15, label='Test Data', color='blue')\n",
    "    \n",
    "    # Plot regression line (extended to test data)\n",
    "    model = models[axis_col]['model']\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    ax.plot(df_test['time_seconds'], y_pred_test, 'r-', \n",
    "            linewidth=2, label='Regression Line', alpha=0.8)\n",
    "    \n",
    "    # Plot threshold lines\n",
    "    min_c = thresholds[axis_col]['MinC']\n",
    "    max_c = thresholds[axis_col]['MaxC']\n",
    "    \n",
    "    ax.axhline(y=y_pred_test.mean() + min_c, color='orange', \n",
    "               linestyle='--', linewidth=1.5, label=f'Alert Threshold', alpha=0.7)\n",
    "    ax.axhline(y=y_pred_test.mean() + max_c, color='red', \n",
    "               linestyle='--', linewidth=1.5, label=f'Error Threshold', alpha=0.7)\n",
    "    \n",
    "    # Mark alerts (only if alerts exist)\n",
    "    if len(df_alerts) > 0 and 'axis' in df_alerts.columns:\n",
    "        axis_alerts = df_alerts[df_alerts['axis'] == f'#{i}']\n",
    "        for idx, alert in axis_alerts.iterrows():\n",
    "            alert_mask = (df_test['timestamp'] >= alert['start_time']) & \\\n",
    "                         (df_test['timestamp'] <= alert['end_time'])\n",
    "            alert_times = df_test[alert_mask]['time_seconds']\n",
    "            alert_values = df_test[alert_mask][axis_col]\n",
    "            \n",
    "            ax.scatter(alert_times, alert_values, color='orange', \n",
    "                      s=50, marker='^', edgecolors='black', linewidths=1.5,\n",
    "                      label='Alert' if idx == axis_alerts.index[0] else '', zorder=5)\n",
    "    \n",
    "    # Mark errors (only if errors exist)\n",
    "    if len(df_errors) > 0 and 'axis' in df_errors.columns:\n",
    "        axis_errors = df_errors[df_errors['axis'] == f'#{i}']\n",
    "        for idx, error in axis_errors.iterrows():\n",
    "            error_mask = (df_test['timestamp'] >= error['start_time']) & \\\n",
    "                         (df_test['timestamp'] <= error['end_time'])\n",
    "            error_times = df_test[error_mask]['time_seconds']\n",
    "            error_values = df_test[error_mask][axis_col]\n",
    "            \n",
    "            ax.scatter(error_times, error_values, color='red', \n",
    "                      s=80, marker='X', edgecolors='black', linewidths=1.5,\n",
    "                      label='Error' if idx == axis_errors.index[0] else '', zorder=6)\n",
    "    \n",
    "    ax.set_title(f'Axis #{i} - Anomaly Detection Results', fontweight='bold')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Current (kWh)')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/anomaly_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "if len(df_alerts) == 0 and len(df_errors) == 0:\n",
    "    print(\"⚠️ No anomalies detected in test data\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"  1. Test data is very clean (within normal variation)\")\n",
    "    print(\"  2. Thresholds may need adjustment\")\n",
    "    print(\"  3. More synthetic anomalies needed in test data generation\")\n",
    "else:\n",
    "    print(\"✓ Final visualization complete with alert/error annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA PROCESSING\")\n",
    "print(f\"   - Training samples: {len(df_train)}\")\n",
    "print(f\"   - Test samples: {len(df_test)}\")\n",
    "print(f\"   - Axes analyzed: 8\")\n",
    "\n",
    "print(\"\\n2. REGRESSION MODELS\")\n",
    "print(f\"   - Models trained: 8 (one per axis)\")\n",
    "print(f\"   - Average R² score: {np.mean([models[f'axis_{i}']['r2'] for i in range(1, 9)]):.4f}\")\n",
    "\n",
    "print(\"\\n3. THRESHOLD METHODOLOGY\")\n",
    "print(\"   - Alert Threshold (MinC): Mean + 2σ\")\n",
    "print(\"   - Error Threshold (MaxC): Mean + 3σ\")\n",
    "print(\"   - Time Window (T): 10 seconds\")\n",
    "print(\"   - Justification: Statistical significance (95% and 99.7% confidence)\")\n",
    "\n",
    "print(\"\\n4. ANOMALY DETECTION RESULTS\")\n",
    "print(f\"   - Total Alerts: {len(df_alerts)}\")\n",
    "print(f\"   - Total Errors: {len(df_errors)}\")\n",
    "\n",
    "if len(df_alerts) > 0:\n",
    "    print(\"\\n   Alerts by Axis:\")\n",
    "    for axis in df_alerts['axis'].unique():\n",
    "        count = len(df_alerts[df_alerts['axis'] == axis])\n",
    "        print(f\"     {axis}: {count} alert(s)\")\n",
    "\n",
    "if len(df_errors) > 0:\n",
    "    print(\"\\n   Errors by Axis:\")\n",
    "    for axis in df_errors['axis'].unique():\n",
    "        count = len(df_errors[df_errors['axis'] == axis])\n",
    "        print(f\"     {axis}: {count} error(s)\")\n",
    "\n",
    "print(\"\\n5. PREDICTIVE MAINTENANCE INSIGHTS\")\n",
    "print(\"   - Early warning system successfully implemented\")\n",
    "print(\"   - Alert system provides preventive maintenance opportunities\")\n",
    "print(\"   - Error detection enables rapid response to critical issues\")\n",
    "print(\"   - Statistical thresholds minimize false positives while capturing anomalies\")\n",
    "\n",
    "print(\"\\n6. DELIVERABLES CHECKLIST\")\n",
    "deliverables = [\n",
    "    (\"README.md\", \"✓\"),\n",
    "    (\"requirements.txt\", \"✓\"),\n",
    "    (\"Training data (CSV)\", \"✓\"),\n",
    "    (\"Test data (CSV)\", \"✓\"),\n",
    "    (\"Database integration\", \"✓\"),\n",
    "    (\"Regression models\", \"✓\"),\n",
    "    (\"Threshold discovery\", \"✓\"),\n",
    "    (\"Alert/Error detection\", \"✓\"),\n",
    "    (\"Visualizations\", \"✓\"),\n",
    "    (\"Logs (alerts/errors)\", \"✓\")\n",
    "]\n",
    "\n",
    "for item, status in deliverables:\n",
    "    print(f\"   {status} {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ PROJECT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review all outputs in the '../outputs/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Close Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "if connection:\n",
    "    connection.close()\n",
    "    print(\"✓ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
